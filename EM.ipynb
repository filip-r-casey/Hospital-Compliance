{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM(dat, k):\n",
    "    if k == 1:\n",
    "        p_class_given_data = np.ones((len(dat), 1))\n",
    "        means = np.array([np.mean(dat.loc[:, \"lat\"]), np.mean(dat.loc[:, \"lng\"])])\n",
    "        covars = np.cov(np.transpose(dat))\n",
    "        p_class = np.array([1])\n",
    "\n",
    "        p_sum = 0\n",
    "        for i in range(len(dat)):\n",
    "            p_sum += p_class_given_data[i][0]*(np.square(dat.iloc[i].values-means[0]))\n",
    "        mean_dist = np.sqrt(p_sum / len(dat.index))\n",
    "        return p_class_given_data, means, covars, p_class, mean_dist\n",
    "\n",
    "    \"\"\"Define the variables we will use in the Algorithm\"\"\"\n",
    "    #This represents Σ_m (covariance matrix of each component) (slide 26)\n",
    "    covars=np.zeros((k,2,2))\n",
    "\n",
    "    #This represents μ_m (mean values of each component) (slide 26)\n",
    "    means=np.zeros((k,2))\n",
    "\n",
    "    #This represents W_m (weights/likelihood of each component) (slide 26 bottom)\n",
    "    p_class=np.zeros(k)\n",
    "\n",
    "    #This represents P(x_i | cluster = m), or Φ(x_i | μ_m, Σ_m) (slide 27)\n",
    "    #During the Expectation step, we will use this to derive p̂_mi (or p_class_given_data) as shown on slide 27\n",
    "    #p_class_given_data will be the same shape\n",
    "    p_data_given_class=np.zeros((len(dat),k))\n",
    "    p_class_given_data=np.zeros((len(dat),k))\n",
    "\n",
    "\n",
    "    \"\"\"Initialize means, covs, p_classes\"\"\"\n",
    "    #initializations of starting points (used to set the initial means below)\n",
    "    init_idx=np.random.choice(range(len(dat)), size=k, replace=False)\n",
    "\n",
    "    #Initialize the covariance matrix, the means, and the p_class for each of k components (dims)\n",
    "    for dim in range(k):\n",
    "        #Set the cov matrix of each component to the cov of the entire dataset (slide 26)\n",
    "        covars[dim,:,:]=np.cov(np.transpose(dat))\n",
    "\n",
    "        #Set initial means to initial chosen data points (slide 26)\n",
    "        means[dim,:]=dat.iloc[init_idx[dim]]\n",
    "\n",
    "        #Give each component equal weighting / likelihood to start (slide 26)\n",
    "        p_class[dim]=1/k\n",
    "\n",
    "    \"\"\"Now Iterate\"\"\"\n",
    "    #Now we start our for loop, where-in we'll alernate between: (1) Expecation Step and (2) Maximization Step\n",
    "    #Note - 50 steps is fine, no requirement to check for convergence on this HW\n",
    "\n",
    "    for step in range(50):\n",
    "        \"\"\"Expectation Step (Slide 27)\"\"\"\n",
    "        for row_i in range(len(dat.index)):\n",
    "            for component in range(k):\n",
    "                p_data_given_class[row_i][component] = stats.multivariate_normal(means[component], covars[component]).cdf(dat.iloc[row_i].values.tolist())*p_class[component]\n",
    "            for component in range(k):\n",
    "                p_class_given_data[row_i][component] = p_data_given_class[row_i][component]/sum(p_data_given_class[row_i])\n",
    "        \"\"\"Maximization Step (Slide 29-30)\"\"\"\n",
    "        for component in range(k):\n",
    "            n_m = sum(p_class_given_data[:, component])\n",
    "            p_class[component] = n_m/len(dat.index)\n",
    "            weighted_values = [[p_class_given_data[idx_x][component]*dat.iloc[idx_x][\"lat\"], p_class_given_data[idx_x][component]*dat.iloc[idx_x][\"lng\"]] for idx_x in range(len(dat.index))]\n",
    "            means[component, :] = list(np.sum(weighted_values, axis=0) / n_m)\n",
    "\n",
    "            covar = [[0, 0],[0, 0]]\n",
    "            x = dat.loc[:, \"lat\"]\n",
    "            y = dat.loc[:, \"lng\"]\n",
    "            for i in range(k):\n",
    "                covar[0][0] += (x[i]-means[component][0])*(x[i]-means[component][0])\n",
    "                covar[0][1] += (x[i]-means[component][0])*(y[i]-means[component][1])\n",
    "                covar[1][0] += (y[i]-means[component][1])*(x[i]-means[component][0])\n",
    "                covar[1][1] += (y[i]-means[component][1])*(y[i]-means[component][1])\n",
    "\n",
    "            covars[component, :, :] = list(np.array(covar)/n_m)\n",
    "    mean_dist = 0\n",
    "    \"\"\"Once we're done with our for loop, we compute the mean dist (This formula is given in the HW below in Part B)\"\"\"\n",
    "\n",
    "    p_sum = 0\n",
    "    for i in range(len(dat.index)):\n",
    "        for component in range(k):\n",
    "            p_sum += p_class_given_data[i][component]*(np.square(dat.iloc[i].values-means[component]))\n",
    "    mean_dist = np.sqrt(p_sum / len(dat.index))\n",
    "    return p_class_given_data, means, covars, p_class, mean_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"postgres\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\",\n",
    ")\n",
    "\n",
    "cur = conn.cursor()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
